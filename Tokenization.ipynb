{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd82484",
   "metadata": {},
   "source": [
    "### perform word and sententence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf55f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"There are multiple ways we can perform tokenization on a given text data. We will be considering most required 5 different types in it. We can choose any method based on language, library and purpose of modeling. Characters like periods, exclamation point(!) and newline char are used to seperate the sentences. But one drawback with split() method, that we can only use one seperator at a time!. It will not charge 1$ also.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d964f",
   "metadata": {},
   "source": [
    "###### 1. tokenization using built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb65184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token: ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'a', 'given', 'text', 'data.', 'We', 'will', 'be', 'considering', 'most', 'required', '5', 'different', 'types', 'in', 'it.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language,', 'library', 'and', 'purpose', 'of', 'modeling.', 'Characters', 'like', 'periods,', 'exclamation', 'point(!)', 'and', 'newline', 'char', 'are', 'used', 'to', 'seperate', 'the', 'sentences.', 'But', 'one', 'drawback', 'with', 'split()', 'method,', 'that', 'we', 'can', 'only', 'use', 'one', 'seperator', 'at', 'a', 'time!.', 'It', 'will', 'not', 'charge', '1$', 'also.']\n",
      "\n",
      "\n",
      "Sentence token: ['There are multiple ways we can perform tokenization on a given text data', ' We will be considering most required 5 different types in it', ' We can choose any method based on language, library and purpose of modeling', ' Characters like periods, exclamation point(!) and newline char are used to seperate the sentences', ' But one drawback with split() method, that we can only use one seperator at a time!', ' It will not charge 1$ also', '']\n"
     ]
    }
   ],
   "source": [
    "# split text by whitespace\n",
    "word=text.split()\n",
    "sent=text.split(\".\")\n",
    "print('Word token:',word)\n",
    "print('\\n')\n",
    "print('Sentence token:', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd4230",
   "metadata": {},
   "source": [
    "###### 2. tokenization using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cceef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token: ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'a', 'given', 'text', 'data', 'We', 'will', 'be', 'considering', 'most', 'required', '5', 'different', 'types', 'in', 'it', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', 'library', 'and', 'purpose', 'of', 'modeling', 'Characters', 'like', 'periods', 'exclamation', 'point', 'and', 'newline', 'char', 'are', 'used', 'to', 'seperate', 'the', 'sentences', 'But', 'one', 'drawback', 'with', 'split', 'method', 'that', 'we', 'can', 'only', 'use', 'one', 'seperator', 'at', 'a', 'time', 'It', 'will', 'not', 'charge', '1', 'also']\n",
      "\n",
      "\n",
      "Sentence token: ['There are multiple ways we can perform tokenization on a given text data', ' We will be considering most required 5 different types in it', ' We can choose any method based on language, library and purpose of modeling', ' Characters like periods, exclamation point(!) and newline char are used to seperate the sentences', ' But one drawback with split() method, that we can only use one seperator at a time!', ' It will not charge 1$ also', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word=re.findall('[\\w]+',text)\n",
    "sent=re.compile('[.]').split(text)\n",
    "print('Word token:',word)\n",
    "print('\\n')\n",
    "print('Sentence token:',sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6384ef3",
   "metadata": {},
   "source": [
    "###### 3. tokenization using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18450f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4662d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9d7abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966484a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token: ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'a', 'given', 'text', 'data', '.', 'We', 'will', 'be', 'considering', 'most', 'required', '5', 'different', 'types', 'in', 'it', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.', 'Characters', 'like', 'periods', ',', 'exclamation', 'point', '(', '!', ')', 'and', 'newline', 'char', 'are', 'used', 'to', 'seperate', 'the', 'sentences', '.', 'But', 'one', 'drawback', 'with', 'split', '(', ')', 'method', ',', 'that', 'we', 'can', 'only', 'use', 'one', 'seperator', 'at', 'a', 'time', '!', '.', 'It', 'will', 'not', 'charge', '1', '$', 'also', '.']\n",
      "\n",
      "\n",
      "Sentence token: ['There are multiple ways we can perform tokenization on a given text data.', 'We will be considering most required 5 different types in it.', 'We can choose any method based on language, library and purpose of modeling.', 'Characters like periods, exclamation point(!)', 'and newline char are used to seperate the sentences.', 'But one drawback with split() method, that we can only use one seperator at a time!.', 'It will not charge 1$ also.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "word=word_tokenize(text)\n",
    "sent=sent_tokenize(text)\n",
    "print('Word token:',word)\n",
    "print('\\n')\n",
    "print('Sentence token:', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa057736",
   "metadata": {},
   "source": [
    "###### 4. tokenization usindg spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ca05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8caee845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m948.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/student/.local/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/student/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/student/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12612ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token: ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'a', 'given', 'text', 'data', '.', 'We', 'will', 'be', 'considering', 'most', 'required', '5', 'different', 'types', 'in', 'it', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.', 'Characters', 'like', 'periods', ',', 'exclamation', 'point', '(', '!', ')', 'and', 'newline', 'char', 'are', 'used', 'to', 'seperate', 'the', 'sentences', '.', 'But', 'one', 'drawback', 'with', 'split', '(', ')', 'method', ',', 'that', 'we', 'can', 'only', 'use', 'one', 'seperator', 'at', 'a', 'time', '!', '.', 'It', 'will', 'not', 'charge', '1', '$', 'also', '.']\n",
      "\n",
      "\n",
      "Sentence token: ['There are multiple ways we can perform tokenization on a given text data.', 'We will be considering most required 5 different types in it.', 'We can choose any method based on language, library and purpose of modeling.', 'Characters like periods, exclamation point(!) and newline char are used to seperate the sentences.', 'But one drawback with split() method, that we can only use one seperator at a time!.', 'It will not charge 1$ also.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(text)\n",
    "word=[]\n",
    "for t in doc:\n",
    "    word.append(t.text)\n",
    "sent=[]\n",
    "for t in doc.sents:\n",
    "    sent.append(t.text)\n",
    "print('Word token:',word)\n",
    "print('\\n')\n",
    "print('Sentence token:',sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc055a9",
   "metadata": {},
   "source": [
    "#### basics of spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db695ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04e89f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token=doc[1]\n",
    "token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56d5edd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa584bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b57feb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b8f57",
   "metadata": {},
   "source": [
    "###### 5. tokenization using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "731b04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f992c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token: ['there', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'a', 'given', 'text', 'data', 'we', 'will', 'be', 'considering', 'most', 'required', '5', 'different', 'types', 'in', 'it', 'we', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', 'library', 'and', 'purpose', 'of', 'modeling', 'characters', 'like', 'periods', 'exclamation', 'point', 'and', 'newline', 'char', 'are', 'used', 'to', 'seperate', 'the', 'sentences', 'but', 'one', 'drawback', 'with', 'split', 'method', 'that', 'we', 'can', 'only', 'use', 'one', 'seperator', 'at', 'a', 'time', 'it', 'will', 'not', 'charge', '1', 'also']\n",
      "\n",
      "\n",
      "Sentence token: ['there are multiple ways we can perform tokenization on a given text data', ' we will be considering most required 5 different types in it', ' we can choose any method based on language', ' library and purpose of modeling', ' characters like periods', ' exclamation point', ' and newline char are used to seperate the sentences', ' but one drawback with split', ' method', ' that we can only use one seperator at a time', ' it will not charge 1', ' also']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "word=text_to_word_sequence(text)\n",
    "sent=text_to_word_sequence(text,split='.')\n",
    "print('Word token:',word)\n",
    "print('\\n')\n",
    "print('Sentence token:',sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c7d9e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "are \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "multiple \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "ways \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "we \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "can \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "perform \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "tokenization \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "on \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "a \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "given \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "text \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "data \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "We \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "will \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "be \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "considering \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "most \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "required \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "5 \n",
      "is_alpha: False \n",
      "is_punct: False \n",
      "like_num: True \n",
      "is_currency: False \n",
      "\n",
      "different \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "types \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "in \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "it \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "We \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "can \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "choose \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "any \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "method \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "based \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "on \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "language \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ", \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "library \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "and \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "purpose \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "of \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "modeling \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "Characters \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "like \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "periods \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ", \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "exclamation \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "point \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "( \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "! \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ") \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "and \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "newline \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "char \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "are \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "used \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "to \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "seperate \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "the \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "sentences \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "But \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "one \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: True \n",
      "is_currency: False \n",
      "\n",
      "drawback \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "with \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "split \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "( \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ") \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "method \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ", \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "that \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "we \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "can \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "only \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "use \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "one \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: True \n",
      "is_currency: False \n",
      "\n",
      "seperator \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "at \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "a \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "time \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "! \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "It \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "will \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "not \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "charge \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "1 \n",
      "is_alpha: False \n",
      "is_punct: False \n",
      "like_num: True \n",
      "is_currency: False \n",
      "\n",
      "$ \n",
      "is_alpha: False \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: True \n",
      "\n",
      "also \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print(t,\"\\nis_alpha:\",t.is_alpha,\n",
    "         \"\\nis_punct:\",t.is_punct,\n",
    "         \"\\nlike_num:\",t.like_num,\n",
    "         \"\\nis_currency:\",t.is_currency,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed8c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
